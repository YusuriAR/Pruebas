{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "wanted-spice",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.read_csv('./1Food_Inspections.csv')\n",
    "df2 = df1.loc[0:5000,].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "accepting-champion",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Función que manda a llamar las trasnformaciones de estandarización de nombre\n",
    "# de variables, ajustes en valores de varia\n",
    "def cleaning(df):\n",
    "    df = c_standarize_column_names(df)\n",
    "    df = c_change_to_lowercase(df, ['aka_name', 'facility_type', 'risk', 'city', 'state', 'inspection_type',\n",
    "                               'results', 'violations'])\n",
    "    df = c_numeric_transformation_risk(df)\n",
    "    df = c_categoric_transformation(df, ['aka_name', 'facility_type', 'inspection_type'])\n",
    "    df = c_date_transformation('inspection_date', df)\n",
    "    df = c_correct_chicago_in_state(df)\n",
    "    df = c_drop_rows_not_chicago(df)\n",
    "    df = c_drop_rows_not_illinois(df)\n",
    "    df = c_drop_rows_nulls_0_in_license(df)\n",
    "    df = c_drop_columns(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Ajusta los nombres de las columnas del dataframe que se pase\n",
    "# como parámetro (entre ellos cambia todo a minísuculas)\n",
    "def c_standarize_column_names(df):\n",
    "    df.columns = df.columns.str.replace(' ', '_')\n",
    "    df.columns = df.columns.str.replace('#', '')\n",
    "    df.columns = df.columns.str.lower()\n",
    "    return df\n",
    "\n",
    "\n",
    "# Ajusta los valores de los variables (columnas) que se pasen como\n",
    "# parámetro del dataframe señalado\n",
    "def c_change_to_lowercase(df, variables):\n",
    "    for name in variables:\n",
    "        df[name] = df[name].str.lower()\n",
    "    return df\n",
    "\n",
    "\n",
    "# Cambiamos la variable categorica de risk a numerica, considerando\n",
    "# que existe un orden natural\n",
    "# Obs: Para la categoría 'All' se le asignó el riesgo máximo de 1\n",
    "#      pero no se tiene un fundamento. En la base se observa que\n",
    "#      la mayoría de estos registros tienen por resultado de auditoría\n",
    "#      ausente, por lo que más bien se trata de un sentinel value\n",
    "def c_numeric_transformation_risk(df):\n",
    "    df['risk'] = df['risk'].replace({'risk 1 (high)': 3, 'risk 2 (medium)': 2, 'risk 3 (low)': 1, 'all': 3})\n",
    "    df['risk'] = df['risk'].convert_dtypes()\n",
    "    return df\n",
    "\n",
    "\n",
    "# Cambia el tipo de columna indicada como 'category'\n",
    "def c_categoric_transformation(df, variables):\n",
    "    for name in variables:\n",
    "        df[name] = df[name].astype(\"category\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# Cambia el tipo de columna indicada como 'date'\n",
    "def c_date_transformation(col, df):\n",
    "    df[col] = pd.to_datetime(df[col])\n",
    "    return df\n",
    "\n",
    "\n",
    "# Homologa el valor de la variable\n",
    "def c_correct_chicago_in_state(df):\n",
    "    df['state'] = df['state'].replace(to_replace=[\"312chicago\", \"cchicago\", \"chchicago\", \"chcicago\", \\\n",
    "                                           \"chicagochicago\", \"chicagohicago\", \"chicagoi\", \\\n",
    "                                           \"chicago.\"], value=\"chicago\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# Función para quitar los registros que no corresponden a chicago\n",
    "def c_drop_rows_not_chicago(df):\n",
    "    df = df[df['city'] == 'chicago']\n",
    "    return df\n",
    "\n",
    "def c_drop_rows_not_illinois(df):\n",
    "    df = df[df['state'] == 'il']\n",
    "    return df\n",
    "\n",
    "\n",
    "# Función para quitar los registros cuyo numero de licencia es cero o\n",
    "# nulo (vacío)\n",
    "def c_drop_rows_nulls_0_in_license(df):\n",
    "    df = df[df['license_'].notnull()]\n",
    "    df = df[df['license_'] != 0]\n",
    "    return df\n",
    "\n",
    "\n",
    "# Se seleccionan las variables deseadas\n",
    "# Obs: Quizá aka_name podría servir, ya que contiene info del tipo de restaurante\n",
    "#      Para pruebas unitarias se podría llenar el zip con los valores de latitud y longitud\n",
    "#      y/o validar la congruencia entre estos 2 datos\n",
    "def c_drop_columns(df):\n",
    "    # df = df.drop(['dba_name', 'aka_name', 'address', 'city', 'state', 'location'], axis=1)\n",
    "    df = df.drop(['city', 'state'], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "interested-cartoon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import numpy as np\n",
    "#from src.etl.cleaning import c_date_transformation\n",
    "\n",
    "# Función que manda a llamar las funciones con las que se crean nuevas\n",
    "# variables\n",
    "def feature_engineering(df):\n",
    "    df = c_date_transformation('inspection_date', df)\n",
    "    fe_add_column_year_of_insp_date(df)\n",
    "    fe_add_column_month_of_insp_date(df)\n",
    "    fe_add_column_day_of_insp_date(df)\n",
    "    fe_add_column_pass_int(df)\n",
    "    #df = fe_add_column_days_since_last_insp(df)\n",
    "    df = fe_add_column_approved_insp(df)\n",
    "    df = fe_add_column_num_viol_last_insp(df)\n",
    "    df = fe_imputer(df)\n",
    "    df = dummies_day(df)\n",
    "    df = dummies_month(df)\n",
    "    fe_add_column_month_of_insp_date(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Se crea variable con el numero de mes en que se hizo la inspección\n",
    "def fe_add_column_year_of_insp_date(df):\n",
    "    df['inspection_year'] = df['inspection_date'].dt.year\n",
    "\n",
    "# Se crea variable con el numero de mes en que se hizo la inspección\n",
    "def fe_add_column_month_of_insp_date(df):\n",
    "    df['inspection_month'] = df['inspection_date'].dt.month\n",
    "\n",
    "\n",
    "# Se crea variable con el numero de semana en que se hizo la inspección\n",
    "def fe_add_column_day_of_insp_date(df):\n",
    "    df['inspection_day'] = df['inspection_date'].dt.dayofweek\n",
    "\n",
    "\n",
    "# Función que crea la variable objetivo que señala con 1 si se pasó la\n",
    "# inspección y 0 en caso contrario\n",
    "# Obs: Se debe considerar 'not ready', 'out of business' o\n",
    "#      'business not located' como 'fail'?\n",
    "def fe_add_column_pass_int(df):\n",
    "    dic_results = {'pass': 1,\n",
    "                   'out of business': 0,\n",
    "                   'no entry': 0,\n",
    "                   'fail': 0,\n",
    "                   'not ready': 0,\n",
    "                   'pass w/ conditions': 1,\n",
    "                   'business not located': 0}\n",
    "    df['pass'] = df.results.map(dic_results)\n",
    "    df['pass'] = df['pass'].convert_dtypes()\n",
    "\n",
    "\n",
    "# Función que crea una variable que señala los días transcurridos\n",
    "# desde la última inspección (requiere que todos los registros\n",
    "# tengan un numero de licencia)\n",
    "# Obs: Toma unos 2.5 minutos en ejecutar\n",
    "def fe_add_column_days_since_last_insp(df):\n",
    "    # Se ordena en forma ascendente por license_ (que se considera representa un mismo individuo)\n",
    "    # y en forma descendente por fecha de inspección, lo cual será de apoyo para recorrer todos los registros\n",
    "    # y hacer cálculos de nuevas variables\n",
    "    df = df.sort_values(['license_', 'inspection_date'], ascending=[True, False])\n",
    "    df = df.reset_index(drop=True)\n",
    "    df['days_since_last_inspection'] = 0\n",
    "    flag = 0\n",
    "    for i in range(df.shape[0]):\n",
    "        if df.iloc[i]['license_'] != flag:\n",
    "            # Cada que observamos un nuevo numero de licencia, estaremos parados en la última inspección\n",
    "            # por lo que calcularemos los días transcurridos del día en que se ejecute este código y la fecha\n",
    "            # de dicha inspección\n",
    "            df.at[i, 'days_since_last_inspection'] = (datetime.now() - df.iloc[i]['inspection_date']).days\n",
    "        else:\n",
    "            # Cuando estamos en esta parte del operador if, estamos en una 2da, 3ra, etc, observación de un mismo\n",
    "            # numero de licencia\n",
    "            df.at[i, 'days_since_last_inspection'] = (\n",
    "                        df.iloc[i - 1]['inspection_date'] - df.iloc[i]['inspection_date']).days\n",
    "        flag = df.iloc[i]['license_']\n",
    "    return df\n",
    "\n",
    "\n",
    "# Función que crea una variable que señala el número de inspecciones anteriores\n",
    "# Obs: Toma unos 2.5 minutos en ejecutar\n",
    "def fe_add_column_approved_insp(df):\n",
    "    df = df.sort_values(['license_', 'inspection_date'], ascending=[True, True])\n",
    "    df = df.reset_index(drop=True)\n",
    "    df['approved_insp'] = 0\n",
    "    flag = 0\n",
    "    for i in range(df.shape[0]):\n",
    "        if df.iloc[i]['license_'] != flag:\n",
    "            df.at[i, 'approved_insp'] = 0\n",
    "        else:\n",
    "            # El conteo de inspecciones pasadas no hace diferencia entre las que fueron aprobadas\n",
    "            # con condiciones o sin condiciones\n",
    "            if df.iloc[i - 1]['results'] == 'pass w/ conditions' or df.iloc[i - 1]['results'] == 'pass':\n",
    "                # si la última inspección fue aprobada se suma 1 al conteo acumulado\n",
    "                df.at[i, 'approved_insp'] = df.iloc[i - 1]['approved_insp'] + 1\n",
    "            else:\n",
    "                # si la última inspección fue reaprobada no se suma nada al conteo acumulado\n",
    "                df.at[i, 'approved_insp'] = df.iloc[i - 1]['approved_insp']\n",
    "        flag = df.iloc[i]['license_']\n",
    "    return df\n",
    "\n",
    "\n",
    "def fe_add_column_num_viol_last_insp(df):\n",
    "    df = df.sort_values(['license_', 'inspection_date'], ascending=[True, True])\n",
    "    df = df.reset_index(drop=True)\n",
    "    df['num_viol_last_insp'] = 0\n",
    "    flag = 0\n",
    "    for i in range(df.shape[0]):\n",
    "        if df.iloc[i]['license_'] != flag:\n",
    "            df.at[i, 'num_viol_last_insp'] = 0\n",
    "        else:\n",
    "            if pd.isnull(df.iloc[i - 1]['violations']):\n",
    "                df.at[i, 'num_viol_last_insp'] = 0\n",
    "            else:\n",
    "                # si el valor de violation de la inspección es no nullo es porque existe una violación (por ello\n",
    "                # de entrada se suma 1), cada violación adicional está señalado separandolo con '| ' (por ello\n",
    "                # se cuentan las veces que aparece dicha secuencia). La violación 60 se refiere a comentarios relativos\n",
    "                # al complimiento de violaciones anteriores ('previous core violation corrected') y como no es una violación\n",
    "                # adicional a la inspección, si se tiene esta nota se resta 1\n",
    "                df.at[i, 'num_viol_last_insp'] = 1 + df.iloc[i - 1]['violations'].count('| ') - df.iloc[i - 1][\n",
    "                    'violations'].count('previous core violation corrected')\n",
    "        flag = df.iloc[i]['license_']\n",
    "    return df\n",
    "\n",
    "\n",
    "def fe_imputer(df):\n",
    "    # Función para imputar columnas específicas\n",
    "    # Obs: Cada variable señalada se imputa con el valor más frecuente. Dada la frecuencia observada\n",
    "    #      en los datos, se puede decir que:\n",
    "    #      - los campos vacíos de 'dba_name' se llenas con 'subway'\n",
    "    #      - los campos vacíos de 'aka_name' se llenas con 'subway'\n",
    "    #      - los campos vacíos de 'facility_type' se llenas con 'restaurant'\n",
    "    #      - los campos vacíos de 'risky' se llenas con 1\n",
    "    #      - El valor que se inserte en los campos vacíos de 'zip' podría no se congruente con 'location'\n",
    "    #        (la mayoría de las veces que no se tiene zip sí se tiene 'location', pero en general son pocos\n",
    "    #        datos faltantes de 'zip')\n",
    "    #      - los campos vacíos de 'inspection_type' se llenas con 'canvass'\n",
    "    #      - los campos vacíos de 'results' se llenas con 'pass'\n",
    "    #      En general no hay datos faltantes en 'inspection_id', 'risk', 'inspection_date', 'results'\n",
    "    transformers = [\n",
    "        ('impute_facility_type', SimpleImputer(strategy=\"most_frequent\"), ['facility_type']),\n",
    "        ('impute_risk', SimpleImputer(strategy=\"most_frequent\"), ['risk']),\n",
    "        ('impute_zip', SimpleImputer(strategy=\"most_frequent\"), ['zip']),\n",
    "        ('impute_inspection_type', SimpleImputer(strategy=\"most_frequent\"), ['inspection_type']),\n",
    "        ('impute_results', SimpleImputer(strategy=\"most_frequent\"), ['results']),\n",
    "        ('impute_latitude', SimpleImputer(strategy=\"most_frequent\"), ['latitude']),\n",
    "        ('impute_longitude', SimpleImputer(strategy=\"most_frequent\"), ['longitude'])\n",
    "    ]\n",
    "    # Definimos el transformador con las transformaciones arriba definidas\n",
    "    col_trans = ColumnTransformer(transformers, remainder=\"passthrough\", n_jobs=-1, verbose=True)\n",
    "    # Ajustamos\n",
    "    col_trans.fit(df)\n",
    "    # Obtenemos el resultado de las transformaciones (imputaciones) aplicadas a la base\n",
    "    aux = col_trans.transform(df)\n",
    "    # Generamos un arreglo auxiliar que contiene los nombres de las variables transformadas\n",
    "    aux_var_imput = pd.DataFrame(transformers[:])[0].str.replace('impute_', '')\n",
    "    # Generamos un arreglo auxiliar que contiene las variables no transformadas\n",
    "    aux_var_no_imput = df.columns[~np.in1d(df.columns, aux_var_imput)]\n",
    "    # Guardamos el orden original de las columnas de la base\n",
    "    col_original_order = df.columns\n",
    "    # creamos un dataframe con los resultados del transformador, plasmando los\n",
    "    # de columnas correspondientes\n",
    "\n",
    "    # creamos un dataframe con las variables dummies y otro dataframe con las variables\n",
    "    # que no transformamos (no dummies) para después unirlos\n",
    "    aux_df_var_no_imput = df[df.columns[~np.in1d(df.columns, aux_var_imput)]]\n",
    "    aux_df_var_imput = pd.DataFrame(aux[:, 0:len(aux_var_imput)], columns=aux_var_imput)\n",
    "    # unimos los 2 dataframes para tener una única base conservando el tipo\n",
    "    # de datos que previamente definimos para las variables que no transformamos\n",
    "    df = pd.concat([aux_df_var_no_imput, aux_df_var_imput], axis=1)\n",
    "    df = df[col_original_order]\n",
    "    return df\n",
    "\n",
    "\n",
    "# def fe_dummier(df):\n",
    "#     transformer = [('one_hot', OneHotEncoder(), ['inspection_month', 'inspection_day'])]\n",
    "#     col_trans = ColumnTransformer(transformer, remainder=\"passthrough\", n_jobs=-1, verbose=True)\n",
    "#     col_trans.fit(df)\n",
    "#     aux = col_trans.transform(df)\n",
    "#     aux_var_dummies = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec', \\\n",
    "#                        'mon', 'tue', 'wed', 'thu', 'fri', 'sat', 'sun']\n",
    "#     # identificamos las variables que no son dummies\n",
    "#     aux_var_no_dummies = df.columns[~np.in1d(df.columns, ['inspection_month', 'inspection_day'])]\n",
    "\n",
    "#     # creamos un dataframe con las variables dummies y otro dataframe con las variables\n",
    "#     # que no transformamos (no dummies) para después unirlos\n",
    "#     aux_df_var_no_dummies = df[df.columns[~np.in1d(df.columns, ['inspection_month', 'inspection_day'])]]\n",
    "#     aux_df_var_dummies = pd.DataFrame(aux[:, 0:len(aux_var_dummies)], columns=aux_var_dummies).convert_dtypes()\n",
    "#     # unimos los 2 dataframes para tener una única base conservando el tipo\n",
    "#     # de datos que previamente definimos para las variables que no transformamos\n",
    "#     df = pd.concat([aux_df_var_no_dummies, aux_df_var_dummies], axis=1)\n",
    "#     return df\n",
    "\n",
    "def dummies_month(df):\n",
    "    d = {\n",
    "        1: 'ene',\n",
    "        2: 'feb',\n",
    "        3: 'mar',\n",
    "        4: 'abr',\n",
    "        5: 'may',\n",
    "        6: 'jun',\n",
    "        7: 'jul',\n",
    "        8: 'ago',\n",
    "        9: 'sep',\n",
    "        10: 'oct',\n",
    "        11: 'nov',\n",
    "        12: 'dic'\n",
    "    }\n",
    "    # Dataframe de ceros con las columnas de todos los meses\n",
    "    df_total_dummies = pd.DataFrame(0, index=np.arange(df.shape[0]), columns=d.values())\n",
    "    # Dataframe con las dummies creadas a partir de los valores encontrados\n",
    "    # (no necesariamente estarían todos los meses)\n",
    "    df_current_dummies = pd.get_dummies(df['inspection_month'])\n",
    "    # Ponemos el nombre de estas dummies acorde al diccionario\n",
    "    # (ejemplo: la columan 1 cambia de nombre a 'ene')\n",
    "    df_current_dummies.columns = df_current_dummies.columns.map(d)\n",
    "    # dataframe auxiliar que une los dos dataframes\n",
    "    aux = pd.concat([df_current_dummies, df_total_dummies], axis=1)\n",
    "    # quitamos las columnas duplicadas, prevaleciendo las que primero encuentra\n",
    "    # que son las current dummies\n",
    "    df_dummies = aux.loc[:,~aux.columns.duplicated()]\n",
    "    df_dummies = df_dummies.astype(int)\n",
    "    # se regresa el dataframe original sin la variable a la que se crearon las dummies,\n",
    "    # uniando al final las columnas dummies\n",
    "    df_new = pd.concat([df.drop(['inspection_month'], axis=1), df_dummies[d.values()]], axis=1)\n",
    "    return df_new\n",
    "\n",
    "def dummies_day(df):\n",
    "    d = {\n",
    "        1: 'mon',\n",
    "        2: 'tue',\n",
    "        3: 'wed',\n",
    "        4: 'thu',\n",
    "        5: 'fri',\n",
    "        6: 'sat',\n",
    "        7: 'sun'\n",
    "    }\n",
    "    # Dataframe de ceros con las columnas de todos los dias de la semana\n",
    "    df_total_dummies = pd.DataFrame(0, index=np.arange(df.shape[0]), columns=d.values())\n",
    "    # Dataframe con las dummies creadas a partir de los valores encontrados\n",
    "    # (no necesariamente estarían todos los dias)\n",
    "    df_current_dummies = pd.get_dummies(df['inspection_day'])\n",
    "    # Ponemos el nombre de estas dummies acorde al diccionario\n",
    "    # (ejemplo: la columan 1 cambia de nombre a 'mon')\n",
    "    df_current_dummies.columns = df_current_dummies.columns.map(d)\n",
    "    # dataframe auxiliar que une los dos dataframes\n",
    "    aux = pd.concat([df_current_dummies, df_total_dummies], axis=1)\n",
    "    # quitamos las columnas duplicadas, prevaleciendo las que primero encuentra\n",
    "    # que son las current dummies\n",
    "    df_dummies = aux.loc[:,~aux.columns.duplicated()]\n",
    "    df_dummies = df_dummies.astype(int)\n",
    "    # se regresa el dataframe original sin la variable a la que se crearon las dummies,\n",
    "    # uniando al final las columnas dummies\n",
    "    df_new = pd.concat([df.drop(['inspection_day'], axis=1), df_dummies[d.values()]], axis=1)\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "curious-mileage",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = cleaning(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "early-retro",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tiempo(archivo,campo_criterio,criterio):\n",
    "    datos_ordenados=archivo.sort_values(by=campo_criterio)\n",
    "    archivo_2=datos_ordenados.loc[datos_ordenados[campo_criterio]<=criterio]\n",
    "    archivo_3=datos_ordenados.loc[datos_ordenados[campo_criterio]>criterio]\n",
    "    return archivo_2, archivo_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "voluntary-blowing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df3.sort_values('inspection_date', inplace = True)\n",
    "train, test = split_tiempo(df3, 'inspection_date', '2020-12-31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "continued-battle",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fe_train = feature_engineering(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "silver-bankruptcy",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = fe_train.drop(['dba_name', 'aka_name', 'facility_type', 'address', 'inspection_date', 'inspection_type', 'violations', 'results', 'location', 'pass'], axis=1)\n",
    "y = fe_train ['pass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-honor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "limited-blackjack",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "particular-amsterdam",
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "cv=[(train_index, test_index)\n",
    "      for train_index,test_index in tscv.split(X)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "appreciated-lotus",
   "metadata": {},
   "outputs": [],
   "source": [
    "def magic_loop(X_train,y_train):\n",
    "    #for i in range(0,len(modelos_to_run)):\n",
    "    classifier = RandomForestClassifier()\n",
    "    hyper_param_grid= {'n_estimators': [100,500,800], \n",
    "                    'max_depth': [1,5,10,50], \n",
    "                    'max_features': ['sqrt','log2'],\n",
    "                    'min_samples_split': [2,5,10],\n",
    "                       'min_samples_leaf':[1,2,4]}\n",
    "    grid_search1 = GridSearchCV(classifier, \n",
    "                           hyper_param_grid, \n",
    "                           scoring = 'f1',\n",
    "                           cv = cv, \n",
    "                           n_jobs = -1,\n",
    "                           verbose = 3)\n",
    "    grid_search1.fit(X_train, y_train)\n",
    "    cv_results = pd.DataFrame(grid_search1.cv_results_)\n",
    "    results_1 = cv_results.sort_values(by='rank_test_score', ascending=True)\n",
    "       \n",
    "        \n",
    "    classifier = tree.DecisionTreeClassifier()\n",
    "    hyper_param_grid= {'max_depth': [3,5,10,25],\n",
    "                       'min_samples_split': [2,5,10,15],\n",
    "                       'min_samples_leaf':[1,2,4],\n",
    "                       'max_features': ['sqrt','log2']}\n",
    "    grid_search2 = GridSearchCV(classifier, \n",
    "                           hyper_param_grid, \n",
    "                           scoring = 'f1',\n",
    "                           cv = cv, \n",
    "                           n_jobs = -1,\n",
    "                           verbose = 3)\n",
    "    grid_search2.fit(X_train, y_train)\n",
    "    cv_results = pd.DataFrame(grid_search2.cv_results_)\n",
    "    results_2 = cv_results.sort_values(by='rank_test_score', ascending=True)\n",
    "\n",
    "    return results_1,results_2\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "generous-uzbekistan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    }
   ],
   "source": [
    "results_1,results_2 =magic_loop(X, y.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "trained-animal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_max_features</th>\n",
       "      <th>param_min_samples_leaf</th>\n",
       "      <th>param_min_samples_split</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0.534688</td>\n",
       "      <td>0.172015</td>\n",
       "      <td>0.03422</td>\n",
       "      <td>0.000976</td>\n",
       "      <td>50</td>\n",
       "      <td>log2</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 50, 'max_features': 'log2', 'min...</td>\n",
       "      <td>0.438406</td>\n",
       "      <td>0.463918</td>\n",
       "      <td>0.414557</td>\n",
       "      <td>0.529703</td>\n",
       "      <td>0.432836</td>\n",
       "      <td>0.455884</td>\n",
       "      <td>0.040146</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "195       0.534688      0.172015          0.03422        0.000976   \n",
       "\n",
       "    param_max_depth param_max_features param_min_samples_leaf  \\\n",
       "195              50               log2                      1   \n",
       "\n",
       "    param_min_samples_split param_n_estimators  \\\n",
       "195                      10                100   \n",
       "\n",
       "                                                params  split0_test_score  \\\n",
       "195  {'max_depth': 50, 'max_features': 'log2', 'min...           0.438406   \n",
       "\n",
       "     split1_test_score  split2_test_score  split3_test_score  \\\n",
       "195           0.463918           0.414557           0.529703   \n",
       "\n",
       "     split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "195           0.432836         0.455884        0.040146                1  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_1.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "tough-creek",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_max_features</th>\n",
       "      <th>param_min_samples_leaf</th>\n",
       "      <th>param_min_samples_split</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.015742</td>\n",
       "      <td>0.004996</td>\n",
       "      <td>0.006013</td>\n",
       "      <td>0.000955</td>\n",
       "      <td>25</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>{'max_depth': 25, 'max_features': 'sqrt', 'min...</td>\n",
       "      <td>0.419872</td>\n",
       "      <td>0.512376</td>\n",
       "      <td>0.577595</td>\n",
       "      <td>0.458716</td>\n",
       "      <td>0.489747</td>\n",
       "      <td>0.491661</td>\n",
       "      <td>0.053022</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "72       0.015742      0.004996         0.006013        0.000955   \n",
       "\n",
       "   param_max_depth param_max_features param_min_samples_leaf  \\\n",
       "72              25               sqrt                      1   \n",
       "\n",
       "   param_min_samples_split                                             params  \\\n",
       "72                       2  {'max_depth': 25, 'max_features': 'sqrt', 'min...   \n",
       "\n",
       "    split0_test_score  split1_test_score  split2_test_score  \\\n",
       "72           0.419872           0.512376           0.577595   \n",
       "\n",
       "    split3_test_score  split4_test_score  mean_test_score  std_test_score  \\\n",
       "72           0.458716           0.489747         0.491661        0.053022   \n",
       "\n",
       "    rank_test_score  \n",
       "72                1  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efficient-great",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distributed-communication",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liquid-detector",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategic-compact",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "mounted-prefix",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo en ejecutar:  176.01030468940735\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# ocuparemos un RF\n",
    "classifier = RandomForestClassifier(oob_score=True, random_state=1234)\n",
    "# separando en train, test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# definicion de los hiperparametros que queremos probar\n",
    "hyper_param_grid = {'n_estimators': [100, 500, 800, 1000], \n",
    "                    'max_depth': [1, 5, 10, 20, 50],\n",
    "                    'min_samples_split': [2, 5, 10]}\n",
    "\n",
    "# ocupemos grid search!\n",
    "gs = GridSearchCV(classifier, \n",
    "                           hyper_param_grid, \n",
    "                           scoring = 'precision',\n",
    "                           cv = 5, \n",
    "                           n_jobs = -1)\n",
    "start_time = time.time()\n",
    "gs.fit(X, y.astype(int))\n",
    "print(\"Tiempo en ejecutar: \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "complex-review",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 100}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "promising-radar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5900753691036479"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "falling-editing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=5, min_samples_split=5, oob_score=True,\n",
       "                       random_state=1234)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "coastal-bowling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6070702966273872"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_estimator_.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mysterious-variable",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-transsexual",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moving-crash",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-hopkins",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earlier-result",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "included-gothic",
   "metadata": {},
   "outputs": [],
   "source": [
    "days_encoder = OneHotEncoder(categories = 'auto')\n",
    "days_encoder.fit(train.inspection_day.values.reshape(train.shape[0],1))\n",
    "days_encoder.transform(train.inspection_day.values.reshape(train.shape[0],1))\n",
    "days_encoder.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bronze-metabolism",
   "metadata": {},
   "outputs": [],
   "source": [
    "month_encoder = OneHotEncoder(categories = 'auto')\n",
    "month_encoder.fit(train.inspection_month.values.reshape(train.shape[0],1))\n",
    "month_encoder.transform(train.inspection_month.values.reshape(train.shape[0],1))\n",
    "month_encoder.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acute-helping",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "#X = train.values.reshape(train.shape[0],19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-encyclopedia",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reliable-assault",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-third",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y.astype(int), random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unsigned-drain",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(random_state=0)\n",
    "tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lined-steps",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorrect-hands",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-magnitude",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phantom-voice",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy on training set: {:.3f}\".format(gs.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(gs.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suffering-columbus",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ongoing-prompt",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "victorian-authorization",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_select = df4[['inspection_id', 'license_', 'risk', 'zip', 'latitude', 'longitude',\n",
    "       'inspection_year', 'approved_insp', 'num_viol_last_insp', 'mon', 'tue',\n",
    "       'wed', 'thu', 'fri', 'sat', 'sun', 'ene', 'feb', 'mar', 'abr', 'may',\n",
    "       'jun', 'jul', 'ago', 'sep', 'oct', 'nov', 'dic', 'inspection_month', 'pass']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blank-caution",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X1_train, X1_test, y1_train, y1_test = split_aleatorio(var_in, var_out, 80,4)\n",
    "X_train, X_test, y_train, y_test = split_tiempo(var_select,'inspection_year',2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "champion-learning",
   "metadata": {},
   "outputs": [],
   "source": [
    "#var_in = df4.drop(['dba_name', 'aka_name', 'facility_type', 'address', 'inspection_date', 'inspection_type', 'violations', 'results', 'location', 'pass'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ideal-bobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "#var_in = df.drop(['pass'], axis=1)\n",
    "#var_out = df['pass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protective-gibson",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attached-monitor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "variance_threshold = VarianceThreshold(threshold=0.1)\n",
    "variance_threshold.fit(var_in)\n",
    "\n",
    "variance_threshold.transform(var_in)\n",
    "\n",
    "variance_threshold.variances_.shape\n",
    "\n",
    "var_in.columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-waste",
   "metadata": {},
   "source": [
    "## Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divided-woman",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_train\n",
    "#y = pd.DataFrame(y_train)\n",
    "y = y_train\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vital-entrance",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "np.random.seed(20201117)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instructional-albuquerque",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "#classifier = RandomForestClassifier(oob_score=True, random_state=1234)\n",
    "tree = DecisionTreeClassifier(random_state=0)\n",
    "tree.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "third-dayton",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlled-nickel",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy on training set: {:.3f}\".format(tree.score(X, y)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, pd.DataFrame(y_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precise-canal",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intense-progress",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinate-steam",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import time \n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# ocuparemos un RF\n",
    "classifier = RandomForestClassifier(oob_score=True, random_state=1234)\n",
    "# separando en train, test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# definicion de los hiperparametros que queremos probar\n",
    "hyper_param_grid = {'n_estimators': [100, 500, 800, 1000], \n",
    "                    'max_depth': [1, 5, 10, 20, 50],\n",
    "                    'min_samples_split': [2, 5, 10]}\n",
    "\n",
    "# ocupemos grid search!\n",
    "gs = GridSearchCV(classifier, \n",
    "                           hyper_param_grid, \n",
    "                           scoring = 'precision',\n",
    "                           cv = 5, \n",
    "                           n_jobs = -1)\n",
    "start_time = time.time()\n",
    "gs.fit(X, y)\n",
    "print(\"Tiempo en ejecutar: \", time.time() - start_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isolated-fight",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-pierce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "allied-spice",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-final",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomForest(train_variables_input,test_variables_input,train_variable_output,numero_arboles,umbral,semilla):\n",
    "    #One hot encoder\n",
    "    #lista_variables_categoricas=train_variables_input.select_dtypes(include = 'object').columns.values\n",
    "    #train_variables_input=pd.get_dummies(train_variables_input,columns=lista_variables_categoricas,prefix=lista_variables_categoricas)\n",
    "    #test_variables_input=pd.get_dummies(test_variables_input,columns=lista_variables_categoricas,prefix=lista_variables_categoricas)\n",
    "    \n",
    "    nombres_columnas=train_variables_input.columns.values\n",
    "    variables_input_array=train_variables_input.values.reshape(train_variables_input.shape[0],train_variables_input.shape[1])\n",
    "    clf = RandomForestClassifier(n_estimators=numero_arboles, random_state=semilla, n_jobs=-1)\n",
    "    #sfm = SelectFromModel(clf, threshold=umbral)\n",
    "    clf.fit(variables_input_array, train_variable_output)\n",
    "    feature_importances = pd.DataFrame(clf.feature_importances_,\n",
    "                                   index = train_variables_input.columns,\n",
    "                                    columns=['importance']).sort_values('importance',ascending=False)\n",
    "    feature_importances=feature_importances.loc[feature_importances['importance']>=umbral]\n",
    "    variables_seleccionadas=list(feature_importances.index)\n",
    "    train_variables_seleccionadas=train_variables_input.loc[:,variables_seleccionadas]\n",
    "    test_variables_seleccionadas=test_variables_input.loc[:,variables_seleccionadas]\n",
    "    return feature_importances,train_variables_seleccionadas,test_variables_seleccionadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configured-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForest(X_train, X_test, y_train, 500, .05, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "macro-hometown",
   "metadata": {},
   "outputs": [],
   "source": [
    "importancia_features,X_train,X_test=feature_selection.RandomForest(X_train,X_test,y_train,500,.05,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historic-oxford",
   "metadata": {},
   "outputs": [],
   "source": [
    "dft = df.sort_values(by=['inspection_year','inspection_month'],ascending=True)\n",
    "#Dividir la base en train y test\n",
    "df_train,df_test = train_test_split(df,test_size=0.2,random_state=1234,shuffle=True)\n",
    "\n",
    "df_input_vars = df_train.drop(['pass'], axis=1)\n",
    "\n",
    "X = df_input_vars\n",
    "y = df_train['pass'].values\n",
    "\n",
    "classifier = RandomForestClassifier(oob_score=True, random_state=1234)\n",
    "\n",
    "# separando en train, test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "# definicion de los hiperparametros que queremos probar\n",
    "hyper_param_grid = {'n_estimators': [10,20], \n",
    "                    'max_depth': [1, 2, 5],\n",
    "                    'min_samples_split': [2, 4]}\n",
    "\n",
    "#Time Series cross-validator\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "gs = GridSearchCV(classifier, \n",
    "                           hyper_param_grid, \n",
    "                           scoring = 'precision',\n",
    "                           cv = tscv,\n",
    "                            \n",
    "                           n_jobs = -1)\n",
    "gs.fit(X_train, y_train) #AQUÍ ESTÁ EL ERROR\n",
    "\n",
    "#Importancia con el mejor modelo\n",
    "importance = gs.best_estimator_.feature_importances_\n",
    "dataset_2 = pd.DataFrame({'importance': importance, 'col_name': names}).sort_values(by='importance',ascending=False)\n",
    "dataset_3 = dataset_2[dataset_2['importance']>=0.07]\n",
    "columnas_modelo = dataset_3['col_name'].values\n",
    "df = pd.DataFrame(X_train.toarray())\n",
    "df.columns = names\n",
    "df_col = df[columnas_modelo]\n",
    "label = pd.DataFrame(y_train,columns=['label'])\n",
    "df_modelo = pd.concat([df_col,label],axis=1)        \n",
    "df_modelo,df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confused-satin",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.unique()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "attached-pastor",
   "metadata": {},
   "source": [
    "def split_tiempo2(archivo,campo_criterio,criterio):\n",
    "    datos_ordenados=archivo.sort_values(by=campo_criterio)\n",
    "    archivo_2=datos_ordenados.loc[datos_ordenados[campo_criterio]<=criterio]\n",
    "    archivo_3=datos_ordenados.loc[datos_ordenados[campo_criterio]>criterio]\n",
    "    X_train=archivo_2.iloc[:, :-1]\n",
    "    y_train=archivo_2.iloc[:, -1]\n",
    "    X_test=archivo_3.iloc[:, :-1]\n",
    "    y_test=archivo_3.iloc[:, -1]\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-grove",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#pd.DataFrame(columns=['X','y'])\n",
    "sr = pd.concat([X,y], axis =1)\n",
    "split_tiempo2(sr,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southwest-defeat",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "cathedral-zoning",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleased-reviewer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respective-employment",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connected-chair",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continent-sponsorship",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranging-nurse",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revolutionary-craps",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-design",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arquitectura",
   "language": "python",
   "name": "arquitectura"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
